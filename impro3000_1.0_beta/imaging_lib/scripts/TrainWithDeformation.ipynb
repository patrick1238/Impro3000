{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.insert(0, 'D:/old_stuff/new_stuff/impro3000-code/branches/impro3000_1.0_beta/imaging_lib')\n",
    "import color_deconvolution as cd\n",
    "import ImageObject as im_o\n",
    "from skimage.measure import regionprops\n",
    "import scipy.ndimage as ndi\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, concatenate, Conv2D, MaxPooling2D, Activation, UpSampling2D, BatchNormalization\n",
    "from keras.optimizers import RMSprop, SGD, Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from os.path import join, isfile\n",
    "from os import listdir\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "from keras.metrics import categorical_accuracy, top_k_categorical_accuracy\n",
    "from scipy.ndimage.interpolation import map_coordinates\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "\n",
    "def make_tensor_one(mask_list):\n",
    "    masks = (np.array(mask_list))\n",
    "    out = np.zeros((masks.shape[0], masks.shape[1], masks.shape[2], 1))\n",
    "    for i in range(masks.shape[0]):\n",
    "            sub_mat = masks[i,:,:]\n",
    "            if 8 in sub_mat:\n",
    "                sub_mat = sub_mat * (sub_mat < 8)\n",
    "            out[i,:,:,0] = sub_mat\n",
    "    print(np.unique(out))\n",
    "    #np.save('img_masks.npy', out)\n",
    "    return  out\n",
    "\n",
    "def get_transformation_matrix(image, alpha, sigma, alpha_affine, random_state=None):\n",
    "    if random_state is None:\n",
    "        random_state = np.random.RandomState(None)\n",
    "\n",
    "    shape = image.shape\n",
    "    shape_size = shape[:2]\n",
    "    \n",
    "    # Random affine\n",
    "    center_square = np.float32(shape_size) // 2\n",
    "    square_size = min(shape_size) // 3\n",
    "    pts1 = np.float32([center_square + square_size, [center_square[0]+square_size, center_square[1]-square_size], center_square - square_size])\n",
    "    pts2 = pts1 + random_state.uniform(-alpha_affine, alpha_affine, size=pts1.shape).astype(np.float32)\n",
    "    M = cv2.getAffineTransform(pts1, pts2)\n",
    "    return M\n",
    "\n",
    "\n",
    "def transform_channel(image, M, alpha, sigma, random_state=None):\n",
    "    shape = image.shape\n",
    "    shape_size = shape[:2]\n",
    "    image = cv2.warpAffine(image, M, shape_size[::-1], borderMode=cv2.BORDER_REFLECT_101)\n",
    "    shape = image.shape\n",
    "    dx = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma) * alpha\n",
    "    dy = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma) * alpha\n",
    "    #dz = np.zeros_like(dx)\n",
    "    #x, y, z = np.meshgrid(np.arange(shape[1]), np.arange(shape[0]), np.arange(shape[2]))\n",
    "    #indices = np.reshape(y+dy, (-1, 1)), np.reshape(x+dx, (-1, 1)), np.reshape(z, (-1, 1))\n",
    "    x, y = np.meshgrid(np.arange(shape[1]), np.arange(shape[0]))\n",
    "    indices = np.reshape(y+dy, (-1,1)), np.reshape(x+dx, (-1,1))\n",
    "    return map_coordinates(image, indices, order=1, mode='reflect').reshape(shape)\n",
    "\n",
    "def elastic_transform(image, mask, alpha, sigma, alpha_affine, random_state=None):\n",
    "    \"\"\"\n",
    "    Taken from: https://www.kaggle.com/bguberfain/elastic-transform-for-data-augmentation\n",
    "    \n",
    "    Elastic deformation of images as described in [Simard2003]_ (with modifications).\n",
    "    .. [Simard2003] Simard, Steinkraus and Platt, \"Best Practices for\n",
    "         Convolutional Neural Networks applied to Visual Document Analysis\", in\n",
    "         Proc. of the International Conference on Document Analysis and\n",
    "         Recognition, 2003.\n",
    "\n",
    "     Based on https://gist.github.com/erniejunior/601cdf56d2b424757de5\n",
    "    \"\"\"\n",
    "    hem_image = image[:,:,0]\n",
    "    cd_image = image[:,:,2]\n",
    "    if random_state is None:\n",
    "        random_state = np.random.RandomState(None)\n",
    "    M = get_transformation_matrix(hem_image, alpha, sigma, alpha_affine, random_state=random_state)\n",
    "    hem_trans = transform_channel(hem_image, M, alpha, sigma, random_state=random_state)\n",
    "    cd_trans = transform_channel(cd_image, M, alpha, sigma, random_state=random_state)\n",
    "    mask_trans = transform_channel(mask,  M, alpha, sigma, random_state=random_state)\n",
    "    out_im = np.zeros(image.shape)\n",
    "    out_im[:,:,0] = hem_trans\n",
    "    out_im[:,:,2] = cd_trans\n",
    "    return out_im, mask_trans\n",
    "    \n",
    "    \n",
    "\n",
    "def deform_image(image, mask, state=None):\n",
    "    width = image.shape[0]\n",
    "    alpha = width * 2\n",
    "    sigma = float(width) * 0.08\n",
    "    alpha_affine = float(width) * 0.08\n",
    "    out_image, out_mask = elastic_transform(image, mask, alpha, sigma, alpha_affine, random_state=state)\n",
    "    return out_image, out_mask\n",
    "\n",
    "def augment_simple(ims, masks, angle_range=90, width_shift_range=30, height_shift_range=30, do_deformation=False):\n",
    "    out_ims = []\n",
    "    out_masks = []\n",
    "    for i in range(len(ims)):\n",
    "        im = ims[i]\n",
    "        mask = masks[i]\n",
    "        height, width = im.shape[:2]\n",
    "        #mask = masks[i]\n",
    "        ### adding un-augmented images to training data\n",
    "        ##out_ims.append(im)\n",
    "        if (np.sum(mask) != 0):\n",
    "            out_ims.append(im)\n",
    "            out_masks.append(mask)\n",
    "            elems = np.unique(mask)\n",
    "            if ((1 in elems) or (2 in elems) or (3 in elems)):\n",
    "                Mrot2 = cv2.getRotationMatrix2D((width / 2, height / 2), int(np.random.uniform(low=0, high=angle_range)), 1)\n",
    "                aug_im3 = cv2.warpAffine(im, Mrot2, (width, height))\n",
    "                aug_mask3 = cv2.warpAffine(mask, Mrot2, (width, height))\n",
    "                #M = np.float32([[1,0,np.random.uniform(low=0.0, high=width_shift_range)],[0,1,np.random.uniform(low=0.0, high=height_shift_range)]])  ### matrix for shifting\n",
    "                #aug_im_h = cv2.warpAffine(im, M, (width, height))\n",
    "                #aug_mask_h = cv2.warpAffine(mask, M, (width, height))\n",
    "                out_ims.append(aug_im3)\n",
    "                out_masks.append(aug_mask3)\n",
    "                #out_ims.append(aug_im_h)\n",
    "                #out_masks.append(aug_mask_h)\n",
    "                if do_deformation:\n",
    "                    im_def, mask_def = deform_image(im, mask)\n",
    "                    out_ims.append(im_def)\n",
    "                    out_masks.append(mask_def)\n",
    "    return out_ims, out_masks\n",
    "\n",
    "def get_train_data(im_folder, mask_folder):\n",
    "    paths_ims = [join(im_folder, f) for f in listdir(im_folder) if isfile(join(im_folder, f))]\n",
    "    paths_masks = [join(mask_folder, f) for f in listdir(mask_folder) if isfile(join(mask_folder, f))]\n",
    "    ims = [(cv2.imread(f).astype(np.float64) / 255.) for f in paths_ims]\n",
    "    ims = [cv2.resize(f, (0,0), fx=0.5, fy=0.5) for f in ims]\n",
    "    masks = [cv2.imread(f, 0) for f in paths_masks]\n",
    "    masks = [cv2.resize(f, (0,0), fx=0.5, fy=0.5) for f in masks]\n",
    "    class_hist = np.zeros(8)\n",
    "    for mask in masks:\n",
    "        for i in range(8):\n",
    "            class_hist[i] += np.sum(mask == i)\n",
    "    print('class histogram is')\n",
    "    print(class_hist * (1./np.sum(class_hist)))\n",
    "    return ims, masks\n",
    "\n",
    "def get_good_means(ims, ans):\n",
    "    val_arrs = [[], [], [], [], [], [], [], []]\n",
    "    means = np.zeros((8))\n",
    "    for im, an in list(zip(ims, ans)):\n",
    "        for i in range(1,9):\n",
    "            mask = an == i\n",
    "            vals = np.extract(mask, im[:,:,2])\n",
    "            val_arrs[i-1] += list(vals)\n",
    "    for i in range(8):\n",
    "        if (len(val_arrs[i]) > 0):\n",
    "            mean = np.mean(val_arrs[i])\n",
    "            means[i] = mean\n",
    "        else:\n",
    "            means[i] = means[i-1] + 5.\n",
    "    print('number of vals in each class is')\n",
    "    for ll in val_arrs:\n",
    "        print(len(ll))\n",
    "    print('means are')\n",
    "    print(means)\n",
    "    return means\n",
    "\n",
    "def transform_mask(im, mask, good_means):\n",
    "    out = np.zeros(mask.shape)\n",
    "    cell_im = (mask == 1) + (mask == 2)\n",
    "    art_im = mask == 5\n",
    "    uns_im = mask == 7\n",
    "    lbl, num = ndi.label(cell_im)\n",
    "    for i in range(1, num):\n",
    "        m_val = np.mean(np.extract(lbl == i, im[:,:,2]))\n",
    "        mean_diffs = np.array([np.linalg.norm(m_val-good_means[0]), np.linalg.norm(m_val-good_means[1]), np.linalg.norm(m_val-good_means[2])])\n",
    "        new_label = np.argmin(mean_diffs) + 1\n",
    "        out += ((lbl == i) * new_label)\n",
    "    lbl, num = ndi.label(art_im)\n",
    "    for i in range(1, num):\n",
    "        m_val = np.mean(np.extract(lbl == i, im[:,:,2]))\n",
    "        mean_diffs = np.array([np.linalg.norm(m_val-good_means[3]), np.linalg.norm(m_val-good_means[4]), np.linalg.norm(m_val-good_means[5])])\n",
    "        new_label = np.argmin(mean_diffs) + 1\n",
    "        out += ((lbl == i) * new_label)\n",
    "    lbl, num = ndi.label(uns_im)\n",
    "    for i in range(1, num):\n",
    "        m_val = np.mean(np.extract(lbl == i, im[:,:,2]))\n",
    "        mean_diffs = np.array([np.linalg.norm(m_val-good_means[6]), np.linalg.norm(m_val-good_means[7])])\n",
    "        new_label = np.argmin(mean_diffs) + 1\n",
    "        out += ((lbl == i) * new_label)\n",
    "    return out\n",
    "            \n",
    "\n",
    "def make_training_images(folder_ims, folder_ans, out_folder_ims, out_folder_ans, index=0):\n",
    "    im_names = [f for f in listdir(folder_ims) if ('.jpg' in f)]\n",
    "    an_names = [f for f in listdir(folder_ans) if ('.tif' in f)]\n",
    "    im_prefixes = []\n",
    "    im_prefixes_good = []\n",
    "    an_prefixes = []\n",
    "    an_prefixes_good = []\n",
    "    for im_name in im_names:\n",
    "        if ('cut' in im_name):\n",
    "            im_prefixes.append(im_name[:im_name.index('cut')])\n",
    "            im_prefixes_good.append(im_name[:im_name.index('cut')])\n",
    "        else:\n",
    "            im_prefixes.append(im_name[:im_name.index('.jpg')])\n",
    "    for an_name in an_names:\n",
    "        if ('_multi' in an_name):\n",
    "            an_prefixes.append(an_name[:an_name.index('_multi')])\n",
    "            an_prefixes_good.append(an_name[:an_name.index('_multi')])\n",
    "        elif ('_an' in an_name):\n",
    "            an_prefixes.append(an_name[:an_name.index('_an')])\n",
    "    ims = []\n",
    "    ans = []\n",
    "    good_ims = []\n",
    "    good_ans = []\n",
    "    print('Fetching data for good means')\n",
    "    for im_pref in im_prefixes_good:\n",
    "        im_index = im_prefixes.index(im_pref)\n",
    "        an_index = an_prefixes.index(im_pref)\n",
    "        im = cv2.imread(join(folder_ims, im_names[im_index]))\n",
    "        im_object = im_o.ImageObject(numpy_array = cv2.cvtColor(im, cv2.COLOR_BGR2RGB))\n",
    "        cd_im, hem = cd.colour_deconvolution(im_object, path='D:/old_stuff/new_stuff/impro3000-code/branches/impro3000_1.0_beta/imaging_lib/color_deconvolution_lib/cd30_hematoxylin_rest.csv')\n",
    "        im_seg = np.zeros(im.shape)\n",
    "        im_seg[:,:,0] = hem.get_numpy_array()\n",
    "        im_seg[:,:,2] = cd_im.get_numpy_array()\n",
    "        im = im_seg\n",
    "        an = cv2.imread(join(folder_ans, an_names[an_index]), 0)\n",
    "        good_ims.append(im)\n",
    "        good_ans.append(an)\n",
    "    print('Got images for good means')\n",
    "    good_means = get_good_means(good_ims, good_ans)\n",
    "    index = 0\n",
    "    \n",
    "    for im_pref in im_prefixes:\n",
    "        print('Processing ' + im_pref)\n",
    "        im_index = im_prefixes.index(im_pref)\n",
    "        an_index = an_prefixes.index(im_pref)\n",
    "        im = cv2.imread(join(folder_ims, im_names[im_index]))\n",
    "        im_object = im_o.ImageObject(numpy_array = cv2.cvtColor(im, cv2.COLOR_BGR2RGB))\n",
    "        cd_im, hem = cd.colour_deconvolution(im_object, path='D:/old_stuff/new_stuff/impro3000-code/branches/impro3000_1.0_beta/imaging_lib/color_deconvolution_lib/cd30_hematoxylin_rest.csv')\n",
    "        im_seg = np.zeros(im.shape)\n",
    "        im_seg[:,:,0] = hem.get_numpy_array()\n",
    "        im_seg[:,:,2] = cd_im.get_numpy_array()\n",
    "        im = im_seg\n",
    "        an = cv2.imread(join(folder_ans, an_names[an_index]), 0)\n",
    "        if (not (im_pref in im_prefixes_good)):\n",
    "            print('Fixing image')\n",
    "            an = transform_mask(im, an, good_means)\n",
    "        print('Unique values are')\n",
    "        print(np.unique(an))\n",
    "        for i in range(0, im.shape[0], 256):\n",
    "            for k in range(0, im.shape[1], 256):\n",
    "                if (((i + 512) <= im.shape[0]) and ((k+512) <= im.shape[1])):\n",
    "                    im_part = np.zeros((512,512,3))\n",
    "                    im_part = np.copy(im[i:i+512,k:k+512])\n",
    "                    an_part = np.zeros((512, 512))\n",
    "                    an_part = np.copy(an[i:i+512,k:k+512])\n",
    "                    im_name = str(index) + '.tif'\n",
    "                    cv2.imwrite(join(out_folder_ims, im_name), im_part)\n",
    "                    cv2.imwrite(join(out_folder_ans, im_name), an_part)\n",
    "                    index += 1\n",
    "    print('Finished writing all images')\n",
    "    return\n",
    "\n",
    "folder = 'D:/old_stuff/new_stuff/slightly_better_training'\n",
    "\n",
    "def prepare_data(im_folder, mask_folder, im_width=256, do_augmentation=True):\n",
    "    num_ims_wrong_shape = 0\n",
    "    norm_params = np.loadtxt('im_mean_std_at3.txt')\n",
    "    im_mean = norm_params[:3]\n",
    "    im_std = norm_params[3]\n",
    "    ims, masks = get_train_data(im_folder, mask_folder)\n",
    "    #im_mean, im_std = #compute_mean_std(ims)\n",
    "    out_arr = np.zeros(4)\n",
    "    out_arr[:3] = im_mean\n",
    "    out_arr[3] = im_std\n",
    "    #np.savetxt('im_mean_std_at3', out_arr)\n",
    "    print('loaded data')\n",
    "    if do_augmentation:\n",
    "        ims, masks = augment_simple(ims, masks, do_deformation=True)\n",
    "    print('augmented data')\n",
    "    class_hist = np.zeros(8)\n",
    "    initial_shape = masks[0].shape\n",
    "    print('initial shape is ')\n",
    "    print(initial_shape)\n",
    "    mask_index = 0\n",
    "    ims_corrected = []\n",
    "    ans_corrected = []\n",
    "    for im, an in list(zip(ims, masks)):\n",
    "        if (an.shape == initial_shape):\n",
    "            ims_corrected.append(im)\n",
    "            ans_corrected.append(an)\n",
    "            for i in range(8):\n",
    "                class_hist[i] += np.sum(an == i)\n",
    "        else:\n",
    "            num_ims_wrong_shape += 1\n",
    "        mask_index += 1\n",
    "    print('found {} images with wrong shapes'.format(num_ims_wrong_shape))\n",
    "    print('class histogram after augmentation is')\n",
    "    print(class_hist * (1./np.sum(class_hist)))\n",
    "    ims = ims_corrected\n",
    "    masks = ans_corrected\n",
    "    sub_im = np.ones((im_width, im_width, 3), dtype=np.float32) * im_mean\n",
    "    masks = make_tensor_one(masks)\n",
    "    ims = [(im - sub_im) / im_std for im in ims]\n",
    "    ims = np.array(ims)\n",
    "    return ims, masks\n",
    "\n",
    "#make_training_images(join(folder, 'big_images'), join(folder, 'big_annotations'), join(folder, 'images'), join(folder, 'annotations'))\n",
    "#ims, masks = prepare_data(join(folder, 'images'), join(folder, 'annotations'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting data preparation\n",
      "class histogram is\n",
      "[9.65233954e-01 2.64906097e-03 7.65291790e-03 2.31224154e-02\n",
      " 5.25843374e-04 7.93934018e-04 9.58767770e-08 2.17784099e-05]\n",
      "loaded data\n",
      "augmented data\n",
      "initial shape is \n",
      "(256, 256)\n",
      "found 0 images with wrong shapes\n",
      "class histogram after augmentation is\n",
      "[9.46141715e-01 5.63586982e-03 1.18188724e-02 3.43689578e-02\n",
      " 8.14366525e-04 1.19016166e-03 1.27219431e-06 2.87840248e-05]\n",
      "[0. 1. 2. 3. 4. 5. 6. 7.]\n",
      "data prepared\n",
      "[2.01592278e-06 3.38430215e-04 1.61381607e-04 5.54962604e-05\n",
      " 2.01592278e-06 2.01592278e-06 2.01592278e-06 2.01592278e-06]\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adadelta\n",
    "from sklearn.utils import class_weight\n",
    "import random\n",
    "\n",
    "def compute_weight_vector(masks):\n",
    "    vector = np.zeros(8)\n",
    "    for i in range(8):\n",
    "        vector[i] = np.sum(masks == i)\n",
    "    weights = float(masks.shape[0]) / (float(8) * vector)\n",
    "    return weights\n",
    "\n",
    "def make_train_test_split(ims, ans, split=0.3):\n",
    "    split_index = int(float(len(ims)) * (1. - split))\n",
    "    split_list = list(zip(ims, list(ans)))\n",
    "    random.shuffle(split_list)\n",
    "    train = split_list[:split_index]\n",
    "    test = split_list[split_index:]\n",
    "    train_ims, train_ans = zip(*train)\n",
    "    test_ims, test_ans = zip(*test)\n",
    "    return train_ims, np.array(train_ans), test_ims, np.array(test_ans)\n",
    "\n",
    "print('starting data preparation')\n",
    "ims, masks = prepare_data(join(folder, 'images'), join(folder, 'annotations'))\n",
    "print('data prepared')\n",
    "\n",
    "w_vec1 = compute_weight_vector(masks)\n",
    "weight_vec_fin = w_vec1\n",
    "for i in range(4,8):\n",
    "    if w_vec1[i] >= np.amin(w_vec1[:4]):\n",
    "        weight_vec_fin[i] = np.amin(w_vec1)\n",
    "print(weight_vec_fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 256)\n",
      "[0. 1. 2. 3.]\n",
      "12138\n",
      "(12138, 256, 256, 1)\n",
      "908\n",
      "(908, 256, 256, 1)\n",
      "Epoch 7/25\n",
      "12138/12138 [==============================] - 3204s 264ms/step - loss: 0.1090 - categorical_accuracy: 0.9619 - val_loss: 0.0668 - val_categorical_accuracy: 0.9914\n",
      "Epoch 8/25\n",
      "12138/12138 [==============================] - 3191s 263ms/step - loss: 0.1055 - categorical_accuracy: 0.9611 - val_loss: 0.0674 - val_categorical_accuracy: 0.9917\n",
      "Epoch 9/25\n",
      "12138/12138 [==============================] - 3192s 263ms/step - loss: 0.1024 - categorical_accuracy: 0.9603 - val_loss: 0.0681 - val_categorical_accuracy: 0.9914\n",
      "Epoch 10/25\n",
      "12138/12138 [==============================] - 3191s 263ms/step - loss: 0.0998 - categorical_accuracy: 0.9597 - val_loss: 0.0745 - val_categorical_accuracy: 0.9947\n",
      "Epoch 11/25\n",
      "12138/12138 [==============================] - 3189s 263ms/step - loss: 0.0966 - categorical_accuracy: 0.9589 - val_loss: 0.0653 - val_categorical_accuracy: 0.9888\n",
      "Epoch 12/25\n",
      "12138/12138 [==============================] - 3186s 262ms/step - loss: 0.0940 - categorical_accuracy: 0.9583 - val_loss: 0.0734 - val_categorical_accuracy: 0.9912\n",
      "Epoch 13/25\n",
      "12138/12138 [==============================] - 3191s 263ms/step - loss: 0.0913 - categorical_accuracy: 0.9577 - val_loss: 0.0777 - val_categorical_accuracy: 0.9944\n",
      "Epoch 14/25\n",
      "12138/12138 [==============================] - 3186s 262ms/step - loss: 0.0882 - categorical_accuracy: 0.9571 - val_loss: 0.0737 - val_categorical_accuracy: 0.9918\n",
      "Epoch 15/25\n",
      "12138/12138 [==============================] - 3199s 264ms/step - loss: 0.0853 - categorical_accuracy: 0.9565 - val_loss: 0.0749 - val_categorical_accuracy: 0.9916\n",
      "Epoch 16/25\n",
      "12138/12138 [==============================] - 3193s 263ms/step - loss: 0.0823 - categorical_accuracy: 0.9559 - val_loss: 0.0874 - val_categorical_accuracy: 0.9807\n",
      "Epoch 17/25\n",
      " 4073/12138 [=========>....................] - ETA: 34:49 - loss: 0.0799 - categorical_accuracy: 0.9556"
     ]
    }
   ],
   "source": [
    "def load_validation_data(im_folder, mask_folder):\n",
    "    paths_ims = [join(im_folder, f) for f in listdir(im_folder) if isfile(join(im_folder, f))]\n",
    "    paths_masks = [join(mask_folder, f) for f in listdir(mask_folder) if isfile(join(mask_folder, f))]\n",
    "    ims = [(cv2.imread(paths_ims[i]).astype(np.float64) / 255.) for i in range(0, len(paths_ims), 2)]\n",
    "    ims = [cv2.resize(f, (0,0), fx=0.5, fy=0.5) for f in ims]\n",
    "    masks = [cv2.imread(paths_masks[i], 0) for i in range(0, len(paths_masks), 2)]\n",
    "    masks = [cv2.resize(f, (0,0), fx=0.5, fy=0.5) for f in masks]\n",
    "    ims_corrected = []\n",
    "    ans_corrected = []\n",
    "    initial_shape = masks[0].shape\n",
    "    im_width = initial_shape[0]\n",
    "    print(initial_shape)\n",
    "    for im, an in list(zip(ims, masks)):\n",
    "        if (an.shape == initial_shape):\n",
    "            ims_corrected.append(im)\n",
    "            ans_corrected.append(an)\n",
    "    norm_params = np.loadtxt('im_mean_std_at3.txt')\n",
    "    im_mean = norm_params[:3]\n",
    "    im_std = norm_params[3]\n",
    "    ims = ims_corrected\n",
    "    masks = ans_corrected\n",
    "    sub_im = np.ones((im_width, im_width, 3), dtype=np.float32) * im_mean\n",
    "    masks = make_tensor_one(masks)\n",
    "    ims = [(im - sub_im) / im_std for im in ims]\n",
    "    ims = np.array(ims)\n",
    "    return ims, masks\n",
    "\n",
    "\n",
    "def train_with_generator(ims, masks, weights, model_path='D:/old_stuff/new_stuff/impro3000-code/branches/impro3000_1.0_beta/imaging_lib/u-net_e18.hdf5', num_epochs=10, start_epoch=0):\n",
    "    \"\"\"\n",
    "    Should work\n",
    "    \"\"\"\n",
    "    \n",
    "    #ims, masks, test_ims, test_ans = make_train_test_split(ims, masks, split=0.2)\n",
    "    #test_ims, test_ans = prepare_data(join(folder, 'validation_images'), join(folder, 'validation_annotations'), do_augmentation=False)\n",
    "    test_ims, test_ans = load_validation_data(join(folder, 'validation_images'), join(folder, 'validation_annotations'))\n",
    "    print(len(ims))\n",
    "    print(masks.shape)\n",
    "    print(len(test_ims))\n",
    "    print(test_ans.shape)\n",
    "    seed = 1\n",
    "    train_generator = ImageDataGenerator()\n",
    "    test_generator = ImageDataGenerator()\n",
    "    train_generator.fit(ims, augment=False)\n",
    "    test_generator.fit(test_ims, augment=False)\n",
    "    if (start_epoch != 0):\n",
    "        if (start_epoch < 10):\n",
    "            model_path = 'D:/old_stuff/new_stuff/u-net_gen_newaug_e0{}.hdf5'.format(start_epoch)\n",
    "        else:\n",
    "            model_path = 'D:/old_stuff/new_stuff/u-net_gen_newaug_e{}.hdf5'.format(start_epoch)\n",
    "    model = load_model(model_path)\n",
    "    model.compile(optimizer=Adam(), loss=sparse_categorical_crossentropy, metrics=[categorical_accuracy])\n",
    "    callbacks = [ModelCheckpoint(filepath='u-net_gen_newaug_e{epoch:02d}.hdf5',\n",
    "                             period=1,\n",
    "                             save_best_only=False,\n",
    "                             save_weights_only=False)]\n",
    "    #history = model.fit_generator(train_generator, validation_data=test_generator, callbacks=callbacks, epochs=num_epochs, verbose=1, class_weight=weights, steps_per_epoch=len(ims)*3, initial_epoch=start_epoch)\n",
    "    if (not weights is None):\n",
    "        history = model.fit_generator(train_generator.flow(ims, masks, batch_size=1), validation_data=test_generator.flow(test_ims, test_ans, batch_size=1), validation_steps=len(test_ims), callbacks=callbacks, epochs=num_epochs, verbose=1, class_weight=weights, steps_per_epoch=len(ims), initial_epoch=start_epoch)\n",
    "    else:\n",
    "        history = model.fit_generator(train_generator.flow(ims, masks, batch_size=1), validation_data=test_generator.flow(test_ims, test_ans, batch_size=1), validation_steps=len(test_ims), callbacks=callbacks, epochs=num_epochs, verbose=1, steps_per_epoch=len(ims), initial_epoch=start_epoch)\n",
    "    np.savetxt('u-net_cat_acc_generator_s{}e{}.txt'.format(start_epoch, num_epochs), history.history['categorical_accuracy'])\n",
    "    np.savetxt('u-net_val_cat_generator_s{}e{}.txt'.format(start_epoch, num_epochs), history.history['val_categorical_accuracy'])\n",
    "    np.savetxt('u-net_loss_generator_s{}e{}.txt'.format(start_epoch, num_epochs), history.history['loss'])\n",
    "    np.savetxt('u-net_val_loss_generator_s{}e{}.txt'.format(start_epoch, num_epochs), history.history['val_loss'])\n",
    "    return\n",
    "\n",
    "\n",
    "train_with_generator(ims, masks, None, num_epochs=25, start_epoch=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
